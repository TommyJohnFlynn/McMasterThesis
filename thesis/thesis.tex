%--------------------------------------------------------------------------------------------%
%       LOADS PACKAGES & CONFIGURES DOCUMENT STYLE        %
%--------------------------------------------------------------------------------------------%
\documentclass[12pt]{report} 
\usepackage{setspace}
\usepackage{amscd,amsmath,amssymb,amsthm} 
\usepackage{lipsum}
\usepackage{epsfig,psfrag,epstopdf} 
\usepackage{fancyhdr} 
\usepackage{lastpage} 
\usepackage{graphicx,wrapfig} 
\usepackage{esvect}
\usepackage{mathtools,cancel}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{hyperref} 
\hypersetup{pdfpagemode=FullScreen, linktoc=section, colorlinks=true}\usepackage{placeins}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{multirow}
\usepackage{algorithm, algpseudocode}
\usepackage{mathrsfs}
\usepackage[thinc]{esdiff}
\usepackage{xcolor}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n} 
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\ra}{rank}
%%%%%%%%%%%%%%%%%%
\topmargin=-.5cm
\evensidemargin=0cm
\oddsidemargin=0cm
\textwidth=16.5cm
\textheight=21.5cm
\headsep=1cm
%%%%%%%%%%%%%%%%%%
\linespread{1.0}
\newcommand{\heading}{\pagestyle{fancy}\lhead{M.Sc. Thesis - Tommy Flynn} \rhead{McMaster - Mathematics and Statistics}\cfoot{\thepage} 
\renewcommand\headrulewidth{0.4pt}
\newcommand{\wkTitle}{An Evolutionary Algorithm for Matrix-Variate Model-Based Clustering} 
\newcommand{\AuthorName}{Tommy Flynn}}
%%%%%%%%%%%%%%%%%%%


\begin{document}   
\setstretch{2}


%------------------------------------------------------%
%      	TABLE OF CONTENTS             %
%------------------------------------------------------%


\pagenumbering{roman} 
\tableofcontents
\listoftables
\listoffigures
\newpage
\pagenumbering{arabic} 
\heading


%---------------------------%
%      CHAPTER 1      %
%---------------------------%


%-------------------------%
\chapter{Introduction}
%-------------------------%

Model-based clustering is the use of finite mixture models to identify underlying group structures in data. A recent review of model-based clustering can be found in \citet{mcnicholas2016b}, while extensive details are available in \citet{mcnicholas2016a}. Estimating parameters for mixture models is notoriously difficult, with the expectation-maximization (EM) algorithm being the predominant method \citep{dempster1977}. Although the EM algorithm is a powerful technique, it is susceptible to becoming trapped at local maxima, which translates to suboptimal clustering results \citep{titterington1985}. An alternative approach is the evolutionary algorithm (EA) which iteratively emulates natural selection on a population of candidate solutions. By leveraging a fitness function and genetic operators like crossover and mutation, EAs offer a distinct way to search the likelihood surface, helping to bypass the limitations of the EM algorithm. 

While EAs have been successful for multivariate model-based clustering \citep{andrews2013,mcnicholas2020}, there remains a paucity of research in the matrix-variate setting. Matrix-variate or three-way data consists of random matrices organized in three dimensions. This structure commonly arises in multivariate longitudinal data, with multiple measurements taken at different time points, and greyscale image data. Model-based clustering has demonstrated its effectiveness in clustering three-way data by leveraging mixtures of matrix-variate distributions \citep{viroli2011, anderlucci2015, dogru2016, gallaugher2018, silva2023}.

In response to the growing interest in matrix-variate data applications, this work develops an EA for matrix-variate model-based clustering. The algorithm incorporates both crossover and mutation operations and is applied in the matrix-variate normal setting. We find that its performance is competitive against the EM algorithm for clustering three-way data in both simulated and real world datasets.   



%---------------------------%
%      CHAPTER 2      %
%---------------------------%



%-------------------------%
\chapter{Background}
%-------------------------%
The following sections provide essential context for this research, covering matrix variate distributions, supervised and unsupervised learning, clustering, and evolutionary computation. Clustering is presented through the model-based paradigm including discussions on finite mixtures of matrix-variate distributions, parameter estimation, convergence criterion, model selection, and model performance. Evolutionary computation is presented with a focus on general evolutionary algorithms, the genetic operators crossover and mutation, and evolutionary algorithms for model-based clustering. 



%---------------------------------------------%
\section{Matrix-Variate Distributions}
%---------------------------------------------%
Three-way data refers to datasets that consist of random matrices organized in three dimensions. These datasets are characterized by having $n$ units (rows), $p$ variables (columns), and $N$ occasions (layers). Matrix-variate distributions provide an effective way to model three-way data, the most mathematically tractable being the matrix-variate normal distribution \citep{gupta1999}. An $n \times p$ random matrix $\mathscr{X}$ follows an $n \times p$ matrix-variate normal distribution, denoted $\mathcal{N}_{n\times p}({\bf M, \Sigma, \Psi})$, if its density can be written as
\begin{equation*}
\phi_{n\times p}({\bf X |  M, \Sigma, \Psi}) =  \frac{1}{(2\pi)^{\frac{np}{2}}|{\bf \Sigma}|^{\frac{p}{2}}|{\bf \Psi}|^{\frac{n}{2}}} \exp\left\{ -\frac{1}{2}\text{tr}\left({\bf \Sigma}^{-1}({\bf X} - {\bf M}) {\bf \Psi}^{-1}({\bf X} - {\bf M}) '\right)\right\},
\end{equation*}
where $\bf M$ is the $n \times p$ location matrix, $\bf \Sigma$ is the $n \times n$ row covariance matrix, and $\bf \Psi$ is the $p \times p$ column covariance matrix. An equivalent formulation in the multivariate setting is given by
\begin{equation*}
\mathscr{X} \sim \mathcal{N}_{n\times p}({\bf M, \Sigma, \Psi}) \iff \text{vec}(\mathscr{X}) \sim \mathcal{N}_{np}(\text{vec}({\bf M}), {\bf \Psi} \otimes {\bf \Sigma}),
\end{equation*}
where $\mathcal{N}_{np}(\cdot)$ is the $np$ multivariate normal distribution, $\text{vec}(\cdot)$ is the vectorization operator, and $\otimes$ is the Kronecker product. A framework for assessing the matrix-variate normality of three-way data using both visual and goodness of fit tests is available in \citet{pocuca2019}. 




%---------------------------------------------------------------------------------------%
\section{Supervised, Unsupervised, and Semi-Supervised Learning}
%---------------------------------------------------------------------------------------%
Machine learning algorithms can be broadly categorized into three varieties: supervised, unsupervised, and semi-supervised. The level of supervision indicates the presence and utilization of labeled data. In regression, labeled data corresponds to the true values of the response variable associated with each input, while in classification, labels indicate group membership. Unsupervised learning is the most general case, where no observations are a priori labeled or are treated as such. In contrast, the other varieties involve some labeled data, which is then used to infer labels for the unlabeled observations. In supervised learning, only the labeled data is used to infer labels for the unlabeled data. Mediating the previous varieties is semi-supervised learning, which leverages all available data to infer labels for the unlabeled data. 



%-----------------------------------------%
\section{Model-Based Clustering}
%-----------------------------------------%



%-----------------------------------------%
\subsection{Historical Perspective}
%-----------------------------------------%
Classification is a paradigm in which group membership labels are assigned to unlabelled data. Unsupervised classification, or clustering, assumes that all observations are unlabelled or are treated as such. The groups to which observations are assigned are referred to as classes or clusters. Defining a cluster formally is surprisingly nontrivial. Intuitively, we want observations within a cluster to exhibit shared characteristics, making them more similar to each other compared to observations in other clusters. 
However, this definition is flawed because it elicits a solution where each observation is assigned to its own cluster \citet{mcnicholas2016a}. 

A more suitable definition can be constructed using finite mixture models. \citet{mcnicholas2016b} explains that the framing of a cluster in terms of a component of a mixture model can be traced back to \citet{tiedeman1955}. Consider a population of $G$ groups where the observations in each group are generated by a Gaussian density function. By censoring the group membership of each observation, we are left with a mixture of unknown densities. The reconstruction of the original $G$ densities is what is known as clustering \citep{tiedeman1955}. Subsequently, \citet{wolfe1963} considered a cluster to be a component of a mixture model and played a pivotal role in pioneering multivariate Gaussian model-based clustering \citep{wolfe1965}. \citet{wolfe1963} also explored two alternative definitions of a cluster. The first definition characterizes a cluster as a mode in a distribution, while the second is based on the similarity between observations. \citet{mcnicholas2016a} synthesizes the historical development of a cluster and offers a refined definition as follows:
\begin{quote}
A cluster is a unimodal component within an appropriate finite mixture model.
\end{quote}
In this context, an appropriate finite mixture model is one that exhibits the necessary flexibility and parameterization to effectively fit the data.




%-----------------------------------------%
\subsection{Finite Mixture Models}
%-----------------------------------------%
The main objective of model-based clustering is maximize the likelihood of a $G$-component finite mixture model. While model-based clustering is typically applied to two-way data, it can be naturally extended to accommodate three-way data using matrix-variate distributions. A matrix-variate random variable $\mathscr{X}$ arises from a finite mixture model if its density can be written as
\begin{equation*}
f({\bf X} | \bm{\vartheta}) = \sum_{g=1}^G \pi_g f_g({\bf X} | \bm{\theta}_g),
\end{equation*}
where $f_g(\cdot)$ is the gth component density, $\pi_g >0$ is the gth mixing proportion with $\sum_{g=1}^G\pi_g = 1$, and $\bm{\vartheta} = (\bm{\pi}, \bm{\theta}_1, \bm{\theta}_2, \dots, \bm{\theta}_G)$ is the vector of parameters with $\bm{\pi} = (\pi_1, \pi_2, \dots, \pi_G)$. In most applications, the component density functions are taken to be identical, indicated as $$f_g({\bf X} | \bm{\theta}_g) = f({\bf X} | \bm{\theta}_g),$$ for all $g$. Building on the ideas in \citet{wolfe1965}, \citet{viroli2011} introduces the use of finite mixtures of matrix-variate normals for clustering three-way data. For the subsequent sections, we denote the density of a mixture of matrix-variate variate normals as
\begin{equation*}f({\bf X} | \bm{\vartheta}) = \sum_{g=1}^G \pi_g \phi({\bf X} | {\bf M}_g, {\bf \Sigma}_g, {\bf \Psi}_g),\end{equation*}
where $\bm{\vartheta} = (\pi_1, \dots, \pi_g, {\bf M}_1, {\bf \Sigma}_1, {\bf \Psi}_1, \dots, {\bf M}_G, {\bf \Sigma}_G, {\bf \Psi}_G)$.




%-----------------------------------------%
\subsection{Parameter Estimation}
%-----------------------------------------%
Estimation of the mixture model parameters is typically performed using the EM algorithm \citep{dempster1977}. In general, the EM algorithm is an iterative procedure for finding maximum likelihood estimates under incomplete data. Consider a dataset comprising $N$ unlabelled matrices ${\bf X}_1, {\bf X}_2, \dots, {\bf X}_N$ of size $n \times p$. Then the observed-data likelihood for a finite matrix-variate normal mixture is given by
\begin{equation*}
\mathbfcal{L}(\bm{\vartheta}) = \prod_{i=1}^N \sum_{g=1}^G\pi_g \phi({\bf X}_i | {\bf M}_g, {\bf \Sigma}_g, {\bf \Psi}_g).
\end{equation*} 
Taking the natural logarithm gives us the observed-data log-likelihood
\begin{equation*}
l(\bm{\vartheta}) = \sum_{i=1}^N \log \left (\sum_{g=1}^G\pi_g \phi({\bf X}_i | {\bf M}_g, {\bf \Sigma}_g, {\bf \Psi}_g)\right).
\end{equation*} 
To complete the data, we introduce latent membership indicators $z_{ig}$ where 
\begin{equation*}
z_{ig} = 
\begin{cases}
    1 & \text{if observation ${\bf X}_i$ belongs to component $g$,} \\
    0 & \text{otherwise},
    \end{cases}
\end{equation*} 
for $i= 1, \dots, N; g = 1, \dots, G$. Under this terminology, the goal of model-based clustering is to accurately predict $z_{ig}$ for each observation and each component. Next, the indicators are organized into row vectors ${\bf z}_1, {\bf z}_2, \dots, {\bf z}_N$ where ${\bf z}_i = (z_{i1}, z_{i2}, \dots, z_{iG})$ denotes the membership label for data point $i$. The collection of all membership labels can be further condensed into an $N \times G$ solution matrix denoting a clustering of the data 
\begin{equation*}
{\bf \mathbfcal{Z}} = 
\begin{bmatrix}
{\bf z}_1 \\
{\bf z}_2 \\
\vdots \\
{\bf z}_N \\
 \end{bmatrix} 
 = 
 \begin{bmatrix}
    z_{11} & z_{12} & \dots & z_{1G} \\
    z_{21} & z_{22}& \dots & z_{2G}\\
    \vdots & \vdots & \ddots & \vdots \\
    z_{N1}& z_{N2}& \dots & z_{NG}\\
\end{bmatrix}.
\end{equation*}
Using the data and membership labels, we obtain the complete-data likelihood and complete-data log-likelihood as follows
\begin{align*}
\mathbfcal{L}_c(\bm{\vartheta}) &= \prod_{i=1}^N \prod_{g=1}^G \left[ \pi_g \phi({\bf X}_i | {\bf M}_g, {\bf \Sigma}_g, {\bf \Psi}_g) \right] ^{z_{ig}}, \\
l_c(\bm{\vartheta}) &= \sum_{i=1}^N \sum_{g=1}^G z_{ig} \left[ \log \pi_g + \log \phi({\bf X}_i | {\bf M}_g, {\bf \Sigma}_g, {\bf \Psi}_g) \right].
\end{align*}
For each iteration of the EM algorithm, the E-step replaces the $z_{ig}$ indicators with their conditional expected values given the data and the current parameter estimates
\begin{equation*}
\hat{z}_{ig} = \mathbb{E}[z_{ig} | {\bf X}_i, \hat{\bm{\vartheta}}] = \mathbb{P}[z_{ig} = 1 | {\bf X}_i, \hat{\bm{\vartheta}}] = \frac{\hat{\pi}_g \phi(\hat{\bf {X}}_i |\hat{\bf {M}}_g, \hat{\bf {\Sigma}}_g, \hat{\bf{\Psi}}_g)}{\sum_{h=1}^G \hat{\pi}_h \phi(\hat{\bf {X}}_i |\hat{\bf {M}}_h, \hat{\bf {\Sigma}}_h, \hat{\bf{\Psi}}_h)}.
\end{equation*}
The predictions can be reported as either soft or hard classifications. In soft classification, the values of $\hat{z}_{ig}$ remain as probabilities in $[0, 1]$, as computed during the E-step. Alternatively, hard classification reports the maximum a posteriori (MAP) classification i.e, $\text{MAP}\{ \hat{z}_{ig}\}$, where 
\begin{equation*}
\text{MAP}\{ \hat{z}_{ig}\} = 
\begin{cases}
    1 & \text{if $g = \text{argmax}_h \{ \hat{z}_{ig}\}$}, \\
    0 & \text{otherwise}.
\end{cases}
\end{equation*}
In the M-step, the parameters are updated. \citet{viroli2011} derives the matrix-variate normal maximum likelihood estimates as
\[
\begin{aligned}
\hat{\pi}_g & = \frac{N_g}{N}, \quad N_g = \sum_{i=1}^N \hat{z}_{ig}, \\
\hat{{\bf M}}_g & = \frac{\sum_{i=1}^N \hat{z}_{ig}{\bf X}_i}{N_g}, \\
\hat{{\bf \Sigma}}_g & = \frac{\sum_{i=1}^N \hat{z}_{ig}({\bf X}_i - \hat{\bf M}_g) \hat{\bf \Psi}_g^{-1}({\bf X}_i - \hat{\bf M}_g)'}{pN_g}, \\
\hat{{\bf \Psi}}_g & = \frac{\sum_{i=1}^N \hat{z}_{ig}({\bf X}_i - \hat{\bf M}_g) \hat{\bf \Sigma}_g^{-1}({\bf X}_i - \hat{\bf M}_g)'}{nN_g},
\end{aligned}
\]
for $g = 1, \dots, G$. The E and M steps iterate until some convergence criterion is satisfied.




%-----------------------------------------%
\subsection{Convergence Criterion}
%-----------------------------------------%
 To determine convergence, we use a criterion based on the Aitken acceleration \citep{aitken1926}. The Aitken acceleration at iteration $t$ is defined as 
\begin{equation*}
a^{(t)} = \frac{l^{(t+1)} - l^{(t)} }{l^{(t)} - l^{(t-1)}},
\end{equation*}
where $l^{(t)}$ is the observed log-likelihood at iteration $t$. From this, \citet{lindsay1995} and \citet{bohning1994} calculate the asymptotic estimate of the log-likelihood at iteration $t+1$ as
\begin{equation*}
l_{\infty}^{(t+1)} = l^{(t)} + \frac{1}{1- a^{(t)}} (l^{(t+1)} - l^{(t)}).
\end{equation*}
As in \citet{mcnicholas2010}, the EM algorithm terminates when 
\begin{equation*}
0< l_{\infty}^{(k+1)} - l^{(k)} < \varepsilon,
\end{equation*}
for some pre-specified tolerance $\varepsilon > 0$.




%------------------------------------%
\subsection{Model Selection}
%------------------------------------%
In general, the number of underlying components or clusters is a priori unknown. To select the appropriate number, we use the Bayesian information criterion \citep[BIC;][]{schwarz1978} defined as 
\begin{equation*}
\text{BIC} = 2 l(\hat{\bm{\vartheta}}) - 2 \rho \log N,
\end{equation*}
where $l(\hat{\bm{\vartheta}})$ is the maximized log-likelihood, $N$ is the number of observations, and $\rho$ is the number of free parameters. The model with the largest BIC value is considered the most appropriate. The use of the BIC for mixture-models has theoretical support in \citet{leroux1992} and \citet{keribin2000} where under certain regularity conditions, the correct number of components is consistently estimated. \citet{dasgupta1998} also demonstrates BIC's value in selecting the appropriate number of components for multivariate Gaussian mixture models. 




%----------------------------------------%
\subsection{Model Performance}
%----------------------------------------%
In the subsequent clustering, labeled datasets are treated as if they were unlabelled. Accordingly, we can assess model performance by comparing MAP classifications to the ground truth. The Rand Index (RI) \citep{rand1971} serves as a metric for quantifying the agreement between two sets of class assignments and is defined as
\begin{equation*}
\text{RI} = \frac{\text{number of pairwise agreements}}{\text{total number of pairs}}.
\end{equation*} 
Pairwise agreement occurs when two observations are correctly assigned labels in relation to each other. If they belong to the same cluster, they should be labeled identically, while if they belong to different clusters, they should receive distinct labels. Conversely, a pairwise disagreement indicates inconsistent labels assigned to two observations. Together, pairwise agreements and disagreements encompass all possible pairs. The RI ranges between 0 and 1, with a value of 1 indicating a perfect match in the assigned classes. Under random assignment, the expected value of the RI is positive. To address this, \citet{hubert1985} proposed the adjusted Rand index (ARI), which scales the RI to correct for the expected number of random pairwise agreements. The ARI is defined as 
\begin{equation*}
\text{ARI} = \frac{\text{RI}  - \mathbb{E}[\text{RI} ]}{\max(\text{RI}) - \mathbb{E}[\text{RI} ]}.
\end{equation*} 
Consequently, the ARI has an expected value of 0 under random assignment and achieves a value of 1 for perfect assignment. It is important to note that negative values are now possible, indicating worse than random assignment.




%--------------------------------------------------%
\subsection{Benefits Over Vectorization}
%--------------------------------------------------%
An alternative approach to matrix-variate clustering is to use vectorization to return to the multivariate setting. However, matrix-variate analysis often offers a substantial reduction in the number of free scale parameters. \citet{gallaugher2018} demonstrates a free scale parameter reduction from $(n^2p^2 + np)/2$ to $(n^2 + p^2 + n + p)/2$, which simplifies and speeds up the procedure for almost all values of $n$ and $p$. 
 
 
 
 
 
%--------------------------------------------%
\section{Evolutionary Computation}
%-------------------------------------------%
Evolutionary computation (EC) is a paradigm inspired by the principles of biological evolution for global optimization. It explores the solution space without assuming any prior knowledge of its structure making it remarkably effective against challenging optimization problems. As an interdisciplinary enterprise, EC has many possible approaches. This treatment focuses on the perspective detailed in \citet{ashlock2010}. 





%-------------------------------------------%
\subsection{Evolutionary Algorithms}
%-------------------------------------------%
An evolutionary algorithm (EA) is a particular form of EC that exploits the concepts of reproduction and natural selection. The goal is to maximize a fitness function by evolving a population of candidate solutions. In contrast to other optimization techniques that yield only a single solution, EAs offer the advantage of obtaining a population of optimal solutions. 

To begin, the first generation of candidate solutions are initialized. For each generation, genetic operators such as crossover and mutation are applied to the population members giving rise to new individuals. Only the fittest individuals among the population survive ensuring the propagation of advantageous traits. This process continues until the population stagnates to a collection of maximally fit solutions.  In terms of natural selection, the traits of the final population have been selected due to their reproductive success. Note that the fitness of a particular solution is determined entirely by the fitness function under consideration. Consequently, a problem may elicit multiple fitness functions resulting in different optimal populations. 




%--------------------------------------%
\subsection{Genetic Operators}
%--------------------------------------%
For reproduction, the principal mechanism is the genetic operator known as crossover. Crossover is the exchange of genetic material between two or more parent solutions to create at least one offspring. The genetic material is exchanged randomly at a single or multiple crossover points giving the offspring different trait combinations. This promotes exploration of the solution space by creating new and diverse candidate solutions. Over many generations, the offspring inherit favourable traits from their parents leading to fitter solutions. Alternatively, mutation is a genetic operator that acts on individual population members by randomly introducing slight genetic modifications. This prevents premature convergence to suboptimal populations that may result from crossover alone. 

The balance between these two operators is crucial for the efficacy of an EA. Natural processes indicate that crossover is to be the principal driving force of the evolutionary process, while mutations are to be applied sparingly. Nonetheless, determining the precise ratio between these operators necessitates parameter tuning to optimize the EA's performance.




%----------------------------------------------------------------------------------%
\subsection{Evolutionary Algorithms For Model-Based Clustering}
%----------------------------------------------------------------------------------%
Early applications of EAs for model-based clustering can be found in \citet{martinez2000} and \citet{pernkopf2005}. From the start, the primary objective has always been to address the local maxima problem associated with the EM algorithm. The original approach was based on evolving the parameters of a multivariate Gaussian mixture model, that is, apply crossover and mutation to the parameters in $\bm{\vartheta}$. The optimization criterion used was the minimum description length (MDL).

In more recent studies, \citet{andrews2013} and \citet{mcnicholas2020} introduced a slightly different approach. Instead of evolving the parameter space, they applied crossover and mutation to the population of latent cluster membership matrices $\mathbfcal{Z}_k$. In this case, the fitness function optimized was the observed-log likelihood, calculated from the parameter updates derived from the data clustering.

The main advantage of evolving the cluster membership labels instead of the model parameters lies in the size of the respective spaces. Data clusterings are a subset of binary matrices of size $G \times N$, making the hard cluster membership space finite, in contrast to the infinite parameter space \citep{andrews2013}.

\citet{mcnicholas2020} successfully developed an EA for multivariate Gaussian model-based clustering, utilizing two-point crossover and a greedy mutation function. In this work, we aim to extend this algorithm to the matrix-variate setting, allowing for broader applicability.



%---------------------------%
%      CHAPTER 3      %
%---------------------------%



%--------------------%
\chapter{Methods}
%--------------------%
The following sections introduce an EA for matrix-variate model-based clustering. First, the underlying model, population, and fitness function are described. Next is a presentation of the algorithm design, including a detailed discussion of the implementations of crossover and mutation. Finally, a procedure for applying the algorithm is provided.




%----------------------------------------------%
\section{Model and Fitness Function}
%----------------------------------------------%
The underlying model for our EA is a mixture of matrix-variate normal distributions. Representing individual clusterings of the data, matrices $\mathbfcal{Z}_k$ consisting of indicator variables $z_{ig}$ serve as population members. Unlike the EM algorithm which uses soft clustering, the EA estimates $z_{ig}$ using hard clustering. For clarity, we denote $\hat{z}_{ig} \in [0, 1]$ for the EM estimates and $\tilde{z}_{ig} \in \{0, 1\}$ for the EA estimates. To compute the fitness of an individual solution, we first update the matrix-variate normal model parameters, replacing $\hat{z}_{ig}$ with $\tilde{z}_{ig}$, that is,
\[
\begin{aligned}
\tilde{\pi}_g & = \frac{N_g}{N}, \quad N_g = \sum_{i=1}^N \tilde{z}_{ig}, \\
\tilde{{\bf M}}_g & = \frac{\sum_{i=1}^N \tilde{z}_{ig}{\bf X}_i}{N_g}, \\
\tilde{{\bf \Sigma}}_g & = \frac{\sum_{i=1}^N \tilde{z}_{ig}({\bf X}_i - \tilde{\bf M}_g) \tilde{\bf \Psi}_g^{-1}({\bf X}_i - \tilde{\bf M}_g)'}{pN_g}, \\
\tilde{{\bf \Psi}}_g & = \frac{\sum_{i=1}^N \tilde{z}_{ig}({\bf X}_i - \tilde{\bf M}_g) \tilde{\bf \Sigma}_g^{-1}({\bf X}_i - \tilde{\bf M}_g)'}{nN_g},
\end{aligned}
\]
for $g = 1, \dots, G$. Then we calculate the observed data log-likelihood at the current EA estimates  
\begin{equation*}
\text{fitness}(\tilde{\bf \mathbfcal{Z}}_k) = \sum_{i=1}^N \log \left (\sum_{g=1}^G\pi_g f({\bf X}_i | \tilde{\bf M}_g, \tilde{\bf \Sigma}_g, \tilde{\bf \Psi}_g)\right).
\end{equation*} 




%--------------------------------%
\section{Algorithm Design}
%--------------------------------%
The EA starts by initializing the first generation with a population of $K$ parent solutions, denoted as $\tilde{\bf \mathbfcal{Z}}_1, \dots, \tilde{\bf \mathbfcal{Z}}_K$. In general, the parent solutions are randomly generated matrices representing random hard clusterings of the data. Alternatively, one can provide initial solutions, such as a hardened $k$-means or other handpicked solutions, to inform the evolutionary process.  

Reproduction in the EA involves cloning the parents $J$ times and performing crossover on each clone. The crossover operation randomly swaps two distinct rows of the parent solution to create clones with identical membership labels except for two observations. For example, suppose we randomly select labels $\tilde{\bf z}_2$ and $\tilde{\bf z}_N$ from the parent in Figure 3.1. Since the rows are distinct, we swap them to create a clone $\tilde{ \bf \mathbfcal{Z} }_{k_j}$ with the same genetic material except for those two labels.  
\begin{figure}[H]
\setstretch{2}
$$\tilde{\bf {\mathbfcal{Z}}}_{k} = 
\begin{bmatrix} \tilde{z}_{11} & \tilde{z}_{12} & \dots & \tilde{z}_{1G} \\
    \textcolor{blue}{0} & \textcolor{blue}{0} & \dots & \textcolor{blue}{1} \\
    \vdots & \vdots & \ddots & \vdots \\
    \textcolor{red}{0}&\textcolor{red}{1}& \dots & \textcolor{red}{0}\\
\end{bmatrix} 
\;
\;
\;
\xrightarrow{\text{crossover}}
\;
\;
\;
\tilde{ \bf \mathbfcal{Z} }_{k_j} = 
\begin{bmatrix}
    \tilde{z}_{11} & \tilde{z}_{12} & \dots & \tilde{z}_{1G} \\
    \textcolor{red}{0} & \textcolor{red}{1} & \dots & \textcolor{red}{0} \\
    \vdots & \vdots & \ddots & \vdots \\
    \textcolor{blue}{0}&\textcolor{blue}{0}& \dots & \textcolor{blue}{1}\\
\end{bmatrix}   $$
\vspace{-0.5cm}
\setstretch{1}
\caption{Illustration of the crossover operator, swapping distinct rows of the parent solution to create a child with similar genetic material.}
\end{figure}

If the labels are identical, we continue randomly selecting rows until two distinct labels are found. After crossover, the population size grows to $K + KJ$, including the original parents and their offspring. The population is then organized into a list of descending fitness. From this list, the top $K$ solutions are selected to become the next generation of parents. This crossover step helps avoid stopping at local maxima of the fitness surface, i.e., the observed log-likelihood. 

Because swapping alone does not guarantee better clustering results, we now introduce a greedy mutation step. For each of the surviving $K$ individuals, we randomly swap distinct elements in a random row until their fitness improves. Suppose we randomly select label $\tilde{\bf z}_2$ from the parent in Figure 3.2. Continuing, we may randomly select distinct elements $\tilde{z}_{22}, \tilde{z}_{2G} \in \tilde{\bf z}_2$ and swap. 
\vspace{0.5cm}
\begin{figure}[H]
\setstretch{2}
$$\tilde{ {\bf \mathbfcal{Z}} }_{k}  = 
\begin{bmatrix}
    \tilde{z}_{11} & \tilde{z}_{12} & \dots & \tilde{z}_{1G} \\
    \tilde{z}_{21} & \textcolor{red}{0} & \dots & \textcolor{blue}{1} \\
    \vdots & \vdots & \ddots & \vdots \\
    \tilde{z}_{N1}& \tilde{z}_{N2}& \dots & \tilde{z}_{NG}\\
\end{bmatrix} 
\;
\;
\;
\xrightarrow{\text{mutation}}
\;
\;
\;
\tilde{ {\bf \mathbfcal{Z}} }_{k} = 
\begin{bmatrix}
    \tilde{z}_{11} & \tilde{z}_{12} & \dots & \tilde{z}_{1G} \\
    \tilde{z}_{21} & \textcolor{blue}{1} & \dots & \textcolor{red}{0} \\
    \vdots & \vdots & \ddots & \vdots \\
    \tilde{z}_{N1}& \tilde{z}_{N2}& \dots & \tilde{z}_{NG}\\
\end{bmatrix}$$
\vspace{-0.5cm}
\setstretch{1}
\caption{Illustration of the mutation operator, swapping distinct elements in a random row of a surviving parent for a slightly modified solution.}
\end{figure}

If the mutation increases fitness, the swap is kept; otherwise, the swap is reversed. This process continues until either a profitable mutation is found or all rows have been exhausted leaving the parent unchanged. If a generation remains unchanged after applying both crossover and mutation, it is considered a stagnation. After a predetermined number of stagnations, the algorithm terminates, resulting in a population of $K$ fit solution matrices.




%-------------------------%
\begin{algorithm}[th]
%-------------------------%
\caption{EA for matrix-variate model-based clustering.}
\begin{algorithmic}[1]
\setstretch{1.15} 
	\Statex \textbf{Input:}
     		\Statex $ {\mathbfcal{X}} = [{\bf X}_1, \dots, {\bf X}_N ]\gets$ $n \times p \times N$ array of observations
		\Statex $ \tilde{ {\bf \mathfrak{Z} }} = [\tilde{ {\mathbfcal{Z}}}_{1}, \dots, \tilde{ {\mathbfcal{Z}}}_{K}]  \gets$ $N \times G \times K$ array of membership labels \Comment{random if not specified}
		 \Statex $G \gets$ number of clusters
   		 \Statex $K \gets$ number of parents
    		 \Statex $J \gets$ number of clones
    		 \Statex $S \gets$ max number of stagnations
\State s = 0
\While{$s < S$}

	\For{$k = 1 \text{ to } K$} \Comment{crossover step}
		\For{$j = 1 \text{ to } J$}
		        \State \textbf{Crossover:} randomly swap two distinct labels from parent $\tilde{ {\bf \mathbfcal{Z}}}_{k}$ to get clone $\tilde{{\bf  \mathbfcal{Z}}}_{k_j}$
       			 \State \textbf{Fitness:} update model parameters and calculate log-likelihood of the clone 
		\EndFor
	\EndFor
	
	\State \textbf{Survival:} sort parents and clones by descending fitness and take top $K$ as new parents 
		
	\For{$k = 1 \text{ to } K$} \Comment{mutation step}
         	\For{$r$ in random permutation of $1$ to $N$} 
        			\State \textbf{Mutate:} swap two distinct elements in row $r$ 
     			\If{fitness increases}
        				\State \textbf{break for}
     			\Else
     				\State swap back 
   			 \EndIf
        		\EndFor
        \EndFor  
        \If{parents are identical to last generation}
        \State \texttt{s} $\gets$  $\texttt{s}  + 1$
        \Else
        \State \texttt{s} $\gets 0$ 
    \EndIf
\EndWhile
\Statex \textbf{Return} final population $\tilde{ {\bf \mathfrak{Z} }}$
\end{algorithmic}
\end{algorithm}
\FloatBarrier 




%-----------------------%
\section{Procedure}
%-----------------------%
The subsequent data analysis adopts the clustering paradigm, treating all membership labels as unknown. For each dataset, two matrix-variate normal mixtures are applied: one using the EM algorithm for parameter estimation and the other using our EA. Both algorithms are initialized with a random start. Let $\mathcal{G}$ be the true number of classes, then algorithms are run for $G=2,\dots, \mathcal{G} + 1$. The value of $G$ which yields the largest BIC is selected. In addition to selecting $G$, the EA must also determine the number of parents $K \in \{1, 2, 3\}$, and the number of clones $J \in \{4, 8, 12\}$. The number of parents is relatively low to ensure that mutations are applied sparingly, while the number of children is large to encourage crossover. For convergence, the EM algorithm uses the Aitken's based criterion with a tolerance of $\varepsilon =10^{-6}$, while the EA uses $S=3$ stagnations. Model performance is based on the final converged log-likelihood, AIC, total runtime, and EA to EM likelihood ratio. The EM algorithm is implemented using the \texttt{R} package \citet{matrixmixtures} and the EA is available in \texttt{Julia} at \citet{flynn2023} \citep{R, Julia}.



%---------------------------%
%      CHAPTER 4      %
%---------------------------%



%------------------------%
\chapter{Simulation}
%------------------------%
In the following sections, two simulations are conducted by clustering a collection of datasets generated from mixtures of matrix-variate normals. In each simulation, a total of 25 datasets are created from the same set of arbitrarily selected model parameters. Simulation 1 involves $3 \times 4$ data consisting of $\mathcal{G}=2$ true classes and a total of $N=300$ observations. The data is drawn to be balanced, with equal proportions in each class $\bm{\pi} = (\frac{1}{2}, \frac{1}{2})$. Simulation 2 considers $4 \times 3$ data generated from $\mathcal{G} = 3$ true classes, totalling $N= 300$ observations. Similarly, the data is balanced, with equal proportions in each class $\bm{\pi} = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})$. The groups in Simulation 1 are well-separated, whereas the groups in Simulation 2 are more challenging to distinguish. 



%------------------------%
\section{Simulation 1}
%------------------------%
In Simulation 1, the location parameters set to
$$
{\bf M}_1 =
\begin{bmatrix} 
1  &  0 & 1  & -1 \\
-1 & -1 &  1 & 0\\
 0 & 0  & 1 & -1
\end{bmatrix}, \quad 
{\bf M}_2 =
\begin{bmatrix} 
0  &  -1 & 1  & 0 \\
-1 & 0 &  0 & 1\\
 1 & 0  & 1 & -1
\end{bmatrix},
$$
and the scale parameters are given by
\begin{align*}
{\bf \Sigma}_1 &=
\begin{bmatrix} 
1  &  0.4 & 0.75   \\
0.4 & 1 &  0\\
 0.75 & 0  & 1 
\end{bmatrix}, & 
{\bf \Sigma}_2 &=
\begin{bmatrix} 
1 &  0.6 & 0.25  \\
0.6 & 1 &  0.1 \\
 0.25 & 0.1  & 1 
\end{bmatrix},
\\
{\bf \Psi}_1 &=
\begin{bmatrix} 
      1  &      0 &   0.35  & 0.15 \\
      0 &       1 &    0      & 0.85\\
 0.35 &       0  &   1      & 0\\
 0.15 &   0.85  &   0     & 1
\end{bmatrix}, & 
{\bf \Psi}_2 &=
\begin{bmatrix} 
      1  &      0.2 &     0  & 0.6 \\
      0.2 &       1 &       0.55      & 0\\
      0 &        0.55  &   1      & 0.3\\
   0.6 &       0  &             0.3     & 1
\end{bmatrix}.
\end{align*}

The EM algorithm selected $G=2$ as the optimal model, whereas the EA identified an optimal model consisting of $G=2$, $K=1$, and $J=12$. Table 4.1 presents the mean and standard deviation for ARI, runtime, and EA to EM likelihood ratio of the optimal models across the 25 runs. The results indicate that both the EM and EA exhibited nearly identical performance. While the EA found slightly superior maxima, as evidenced by the average likelihood ratio, the EM achieved slightly better ARI scores. We acknowledge that the EA is generally slower than the EM due to its more computationally intensive nature; however, the runtimes appear comparable in this case since the number of parents is quite low.

\begin{table}[!htbp]
  \caption{Mean and standard deviation of ARI, runtime, and likelihood ratio for EA and EM associated with Simulation 1.}
  \setstretch{1.25}
\vspace{0.25cm}
  \begin{tabularx}{\textwidth}{l *{3}{X}c}
  \toprule
    &\textbf{ARI}& \textbf{Runtime (sec)}  & \textbf{Likelihood Ratio} \\
  \midrule
  EA & 0.992 (0.010)     & 1.56 (0.19) & \multirow{2}{*}{1.001 (0.005)}  \\
  EM & 0.993 (0.008)    & 0.77 (0.06)   \\
  \bottomrule
  \end{tabularx}
\end{table}




%------------------------%
\section{Simulation 2}
%------------------------%
The location parameters for simulation 2 are given by
$$
{\bf M}_1 =
\begin{bmatrix} 
0  &  0.5 & 1   \\
0.5 & 1 &  0.5 \\
0.5 & 1 &  0.5 \\
 0 & 1 & 0
\end{bmatrix}, \quad 
{\bf M}_2=
\begin{bmatrix} 
1 &  0.5 & 0   \\
1.5 & 1 &  2 \\
0 & 2 &  0.5 \\
 1.5 & 0.5 & 1
\end{bmatrix},
\quad 
{\bf M}_3=
\begin{bmatrix} 
1.5 &  2.5 & 2   \\
1 & 3 &  1.5 \\
0.5 & 3 &  1.5 \\
 1.5 & 0.5 & 1
\end{bmatrix},
$$
and the scale parameters configured to
\begin{align*}
{\bf \Sigma}_1 = {\bf \Sigma}_3 &=
\begin{bmatrix} 
1 &     0.1&     0.45&        0.1\\
                0.1&     1&     0.25&    0.35\\
                0.45&       0.25&     1&     0.1\\
                0.1&      0.35&       0.1&       1\\
\end{bmatrix},&  
{\bf \Sigma}_2 &=
\begin{bmatrix} 
      1  &      0.2 &     0  & 0.6 \\
      0.2 &       1 &       0.55      & 0\\
      0 &        0.55  &   1      & 0.3\\
   0.6 &       0  &             0.3     & 1
\end{bmatrix},
\\
{\bf \Psi}_1 &=
\begin{bmatrix} 
1  &  0.4 & 0.75   \\
0.4 & 1 &  0\\
 0.75 & 0  & 1 
\end{bmatrix},  &
{\bf \Psi}_2= {\bf \Psi}_3 &= 
\begin{bmatrix} 
1  &  0.5 & 0.5   \\
0.5 & 1 &  0\\
 0.5 & 0.5  & 1 
\end{bmatrix}.
\end{align*}
The optimal model selected by the EM algorithm was $G=3$, while the EA identified an optimal model consisting of $G=3$, $K=3$, and $J=12$. Table 4.2 presents the mean and standard deviation for ARI, runtime, and EA to EM likelihood ratio of the optimal models across the 25 runs. The results are similar to the previous simulation, with the EA exhibiting marginally superior performance in terms of likelihood, while the EM slightly outperformed in ARI scores. Notably, the larger number of parents resulted in significantly longer runtimes due to the greedy nature of the mutation step.

\begin{table}[!ht]
  \caption{Mean and standard deviation of ARI, runtime, and likelihood ratio for EA and EM associated with Simulation 2.}
\setstretch{1.5}
    \vspace{0.5cm}
  \begin{tabularx}{\textwidth}{l *{3}{X}c}
  \toprule
    &\textbf{ARI}& \textbf{Runtime (sec)}  & \textbf{Likelihood Ratio} \\
  \midrule
  EA & 0.930 (0.032)     & 14.72 (1.82) & \multirow{2}{*}{1.041 (0.101)}  \\
  EM & 0.942 (0.026)    & 1.83 (0.42)   \\
  \bottomrule
  \end{tabularx}
\end{table}



%---------------------------%
%      CHAPTER 5      %
%---------------------------%



%------------------------%
\chapter{Application}
%------------------------%
The following sections detail an investigation of two real world datasets. First is the Landsat satellite dataset, which involves $3 \times 3$ digital images of the same regions taken in four different spectral bands. The pixels are arranged into matrices of size $4 \times 9$ and three classes are used. The second dataset contains $16 \times 16$ greyscale images of handwritten digits. For clustering, we focus on digits 1 and 7.  




%-------------------------------------------%
\section{Landsat Satellite Dataset}
%-------------------------------------------%
This dataset is a collection of digital images captured by the Landsat program, which is a series of Earth-observing satellites managed by NASA and the United States Geological Survey (USGS). The original data was purchased from NASA by the Australian Centre for Remote Sensing and subsequently donated to the UCI machine learning repository in a preprocessed form \citep{landsat}. The preprocessing includes converting the original binary values to ASCII, labelling the data based on site visits, removing data to prevent image reconstruction, and partitioning the data into training and testing sets. In this dataset, four digital images of size $3 \times 3$ were taken of the same region in different spectral bands, corresponding to random matrices of size $4\times 9$. Two of the bands were in the visible spectrum, while two were in the infrared spectrum. The pixel intensities range from 0 to 255. The researchers identified seven classes of images based on the central pixels which are described in Table 5.1. Notably, class 6 was removed by the researchers due to doubts of the validity of the class.

\begin{table}[!htbp]
  \caption{Description of the Landsat test set classes and observation count.}
  \setstretch{1.5}
  \vspace{0.5cm}
  \begin{tabularx}{\textwidth}{cXc}
    \toprule
    \textbf{Class} & \textbf{Description} & \textbf{Count} \\
    \midrule
    1 & Red soil                        & 461 \\
    2 & Cotton crop                     & 224 \\
    3 & Grey Soil                       & 396 \\
    4 & Damp grey soil                  & 211 \\
    5 & Soil with vegetation stubble    & 237 \\
    6 & Mixture of all classes          & 0 \\
    7 & Very damp grey soil             & 470 \\
    \bottomrule
  \end{tabularx}
\end{table}

Originally, this dataset was used for multivariate clustering by considering only a vector of the 4 central pixels, which is what the labels are based on. However, this approach neglects a significant amount of information along the borders which can be leveraged by matrix-variate distributions. Although we possess more information, the regions along the boundaries between classes are less distinct and pose a greater challenge to separate.
 
For our analysis, we focused on the first three classes from the test set: red soil, cotton crop, and grey soil, resulting in a dataset of $N = 1081$ observations with true proportions approximately $\bm{\pi} = (0.43, 0.21, 0.37)$. Surprisingly, both the EM and EA selected $G=4$ classes using the BIC. The presence of this extra class could be attributed to the lack of distinct borders between classes of images, as the images were labeled based on their central pixels. For the EA, the BIC selected an optimal model of $K=2$ parents and $J=8$ clones. Table 5.2 showcases the results, including the log-likelihood, ARI, total runtime, and the EA to EM likelihood ratio. Moreover, Tables 5.3 and 5.4 present the cross-tabulation of the MAP classifications for the EA and EM, respectively. The results demonstrate that, despite an increase of approximately 5 minutes in runtime, the EA discovered a superior maximum. An EA to EM likelihood ratio of approximately 1.55 indicates a substantial improvement in likelihood. This increase in EA likelihood translated to a slight increase in ARI over the EM. Notably, both algorithms exhibit a similar clustering pattern in the cross-tabulations.


\newpage

\begin{table}[!htbp]
  \caption{Converged log-likelihood, ARI, runtime, and EA to EM likelihood ratio associated with the Landsat dataset.}
  \setstretch{1.5}
    \vspace{0.5cm}
  \begin{tabularx}{\textwidth}{l *{3}{X}c}
  \toprule
    &\textbf{Log-Likelihood}& \textbf{ARI} & \textbf{Runtime (sec)} & \textbf{Likelihood Ratio} \\
  \midrule
   EA & -108118.26     & 87.8 &  348.48 & \multirow{2}{*}{1.55}\\
  EM & -108118.7          & 86.9 & 36.89    \\
  \bottomrule
  \end{tabularx}
\end{table}

\begin{table}[!htbp]
  \caption{Cross-tabulation of the EA MAP classifications associated with the Landsat dataset.}
  \setstretch{1.5}
    \vspace{0.5cm}
  \begin{tabularx}{\textwidth}{l *{4}{X}}
  \toprule
   &\textbf{Cluster 1}  & \textbf{Cluster 2}  & \textbf{Cluster 3}  & \textbf{Cluster 4}   \\
  \midrule
  Red Soil & 	452 & 0 & 0 & 9\\
  Cotton Crop &0 & 140 &  0 & 84 \\
   Grey Soil &   7 & 0 &  368  & 21\\
  \bottomrule
  \end{tabularx}
\end{table}

\begin{table}[!htbp]
  \caption{Cross-tabulation of the EM MAP classifications associated with the Landsat dataset.}
  \setstretch{1.5}
    \vspace{0.5cm}
  \begin{tabularx}{\textwidth}{l *{4}{X}}
  \toprule
   &\textbf{Cluster 1}  & \textbf{Cluster 2}  & \textbf{Cluster 3}  & \textbf{Cluster 4}   \\
  \midrule
  Red Soil & 	450 & 0 & 0 & 11\\
  Cotton Crop &0 & 150 &  0 & 74 \\
   Grey Soil &   7 & 0 &  363  & 26\\
  \bottomrule
  \end{tabularx}
\end{table}

\newpage


%--------------------------------------------%
\section{Handwritten Digits Dataset}
%--------------------------------------------%
The final dataset comprises greyscale images of handwritten digits, which were obtained by scanning envelopes from the U.S postal service. The original data is in binary format, and the images vary in size and orientation. To facilitate analysis, the Elements of Statistical Learning provides preprocessed training and test sets by centering, reorienting, resizing to $16 \times 16$, and normalizing the pixel intensities between -1 and 1 \citep{hastie2009}. 

Since the writing is mostly concentrated in the center, the outer rows and columns primarily contain white space, resulting in a value of -1. Consequently, the lack of variation along the borders leads to singular updates in the scale matrices ${\bf \Sigma}_g$ and ${\bf \Psi}_g$. To address this sparsity, we introduced small random noise to the images. However, to preserve the signal integrity, a constant is applied to all entries greater than -1 before adding the noise.

Our analysis focused on clustering digits 1 and 7 from the dataset. There were a total of $N=411$ observations from the test set, with 264 belonging to digit 1 and 147 to digit 7, corresponding to true proportions of approximately $\bm{\pi} = (0.64, 0.36)$. The EM algorithm identified the optimal number of classes as $G=2$, while the EA selected $G=2, K=2,$ and $J=4$. In Table 5.5, we present the log-likelihood, ARI, runtime, and EA to EM likelihood ratio. Additionally, Tables 5.6 and 5.7 display the cross-tabulation of EA and EM results, respectively.

Remarkably, both algorithms showed identical clustering results in terms of likelihood and ARI. The cross-tabulations for EA and EM were also identical. However, we note that the EA required significantly more time to achieve the same results as the EM algorithm.



\begin{table}[!htbp]
  \caption{Converged log-likelihood, ARI, runtime, and EA to EM likelihood ratio associated with the handwritten digits dataset.}
  \setstretch{1.5}
    \vspace{0.5cm}
  \begin{tabularx}{\textwidth}{l *{3}{X}c}
  \toprule
    &\textbf{Log-Likelihood}& \textbf{ARI} & \textbf{Runtime (sec)} & \textbf{Likelihood Ratio} \\
  \midrule
  EA & -97320.28     & 90.4 &  43.75 & \multirow{2}{*}{1.00}  \\
  EM & -97320.28    & 90.4 & 2.95  \\
  \bottomrule
  \end{tabularx}
\end{table}


\begin{table}[!htbp]
  \caption{Cross-tabulation of the EA MAP classifications associated with the handwritten digits dataset.}
  \setstretch{1.5}
    \vspace{0.5cm}
  \begin{tabularx}{\textwidth}{l *{4}{X}}
  \toprule
   && \textbf{Cluster 1}  & \textbf{Cluster 2}  \\
  \midrule
  Digit 1 && 256 & 8 \\
  Digit 7 && 2 &  145 \\
  \bottomrule
  \end{tabularx}
\end{table}

\begin{table}[!ht]
  \caption{Cross-tabulation of the EM MAP classifications associated with the handwritten digits dataset.}
  \setstretch{1.5}
    \vspace{0.5cm}
  \begin{tabularx}{\textwidth}{l *{4}{X}}
  \toprule
   && \textbf{Cluster 1}  & \textbf{Cluster 2} \\
  \midrule
  Digit 1 && 256 & 8 \\
  Digit 7 && 2 &  145 \\
  \bottomrule
  \end{tabularx}
\end{table}



%---------------------------%
%      CHAPTER 6      %
%---------------------------%


%-------------------------%
\chapter{Conclusions}
%-------------------------%




%------------------------%
\section{Discussion}
%------------------------%
This work has extended the current literature on EAs for model-based clustering by incorporating mixtures of matrix-variate distributions. In particular, an EA was developed for matrix-variate normal mixtures using crossover and mutation. The EA was applied to simulated and real world datasets and its performance was compared against the EM algorithm. 

Simulation 1 featured datasets with well-separated groups generated by predetermined matrix-variate normal distributions. In Simulation 2, the datasets comprised additional groups that were more challenging to separate. Both the EA and EM algorithm performed similarly well on these datasets, with the EA showing a slightly superior likelihood performance and the EM algorithm having slightly better ARI performance.

Moving to real-world datasets, the Landsat satellite dataset presented a more difficult clustering problem due to the non-distinct nature of the classes and the lack of clear borders between images. Here, the EA outperformed the EM algorithm in terms of both likelihood and ARI, showcasing its potential to handle complex and ill-defined problems more effectively. The second real-world dataset, containing handwritten digits of 1's and 7's, had much higher dimensions than the previous datasets. In this case, both the EA and EM performed equally well based on likelihood and ARI, although the EA struggled with runtime. 

Overall, the EA proves to be a competitive alternative for fitting model parameters in matrix-variate normal mixtures, consistently performing at least as well as the EM algorithm in terms of likelihood and ARI. However, its limitation lies in its runtime, which is understandable given its approach of finding multiple solutions and the nature crossover and mutation operators. Considering the results obtained, it is recommended to deploy the EM algorithm in most cases, as it is computationally more efficient. The EA should be reserved for scenarios where the problem complexity is particularly high.




%-------------------------%
\section{Future Work}
%-------------------------%
Future research could focus on improving the EA's runtime performance, either by exploring more optimal implementations of genetic operators, using parallel computing, or by leveraging lower-level languages such as \texttt{C}. Additionally, extending the investigation to non-Gaussian distributions could broaden the applicability of the proposed method. Lastly, exploring new genetic operators or modified versions of crossover and mutation could lead to further enhancements in the EA's performance.




%-------------------------------%
%      BIBLIOGRAPHY     %
%-------------------------------%
\clearpage
\newpage
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{ref.bib}
 \end{document}