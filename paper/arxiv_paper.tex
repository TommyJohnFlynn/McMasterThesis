%--------------------------------------------------------------------------------------------%
%       LOADS PACKAGES & CONFIGURES DOCUMENT STYLE        %
%--------------------------------------------------------------------------------------------%
\documentclass[12pt, centerh1]{article}
\textwidth=165mm \headheight=0mm \headsep=10mm \topmargin=0mm
\textheight=220mm %\footskip=1.5cm
\oddsidemargin=0mm
%\documentclass[12pt,letterpaper]{article}
%\usepackage[margin=1in]{geometry}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}
\usepackage{amscd,amsmath,amssymb,amsthm, natbib} 
\usepackage{mathrsfs} 
\usepackage[nodisplayskipstretch]{setspace}
%\usepackage[mathscr]{euscript}
%\usepackage{mathrsfs}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx,bm}
\usepackage{setspace}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n} 
\usepackage{color}
\usepackage{algorithm, algpseudocode}
\usepackage{subcaption}
 \usepackage[table]{xcolor}
\usepackage{longtable}
\usepackage{amsthm}
\usepackage{relsize}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
\usepackage{rotating}
\usepackage{eurosym}
\usepackage{colonequals}
\usepackage{bbm}

%\usepackage{titling}
%\usepackage{lipsum}



%-------------------%
%      TITLE      %
%------------------%
\title{An Evolutionary Algorithm for Matrix-Variate Model-Based Clustering}
\author{\qquad Tommy Flynn \qquad\qquad\ Paul D.\ McNicholas}
\date{{\small Department of Mathematics and Statistics, McMaster University, Ontario, Canada.}}


\newcommand{\bluey}[1]{\textcolor{brown}{#1}}
%%%%%%%%%%%%%%%
\setstretch{1}
\pdfminorversion=4

\begin{document}

\maketitle

\begin{abstract}
An evolutionary algorithm (EA) is developed as an alternative to the expectation-maximization (EM) algorithm for parameter estimation in matrix-variate model-based clustering. Using the principles of biological evolution, EAs offer a distinct way to search the likelihood surface, helping to avoid local maxima. EAs have been developed for multivariate model-based clustering, however, there remains a paucity of research in the matrix-variate setting. In response to the growing interest in matrix-variate data applications, this work develops a crossover and mutation based EA for mixtures of matrix-variate normals. We find that its performance is competitive against the EM algorithm for clustering three-way data in both simulated and real-world datasets. \\[-10pt]

\noindent\textbf{Keywords}: clustering; mixture models; evolutionary algorithm; matrix-variate; three-way data; crossover; mutation.
\end{abstract}
\newpage


%---------------------------%
%      CHAPTER 1      %
%---------------------------%
\section{Introduction}
Model-based clustering is the use of finite mixture models to identify underlying group structures in data. Estimating parameters for mixture models is primarily accomplished using the expectation-maximization (EM) algorithm \citep{dempster1977}; however; it is susceptible to becoming trapped at local maxima \citep{titterington1985}. An alternative approach is the evolutionary algorithm (EA) which emulates natural selection on a population of candidate solutions. By leveraging a fitness function and genetic operators like crossover and mutation, EAs offer a distinct way to search the likelihood surface.

While EAs have been developed for multivariate model-based clustering \citep{andrews2013,mcnicholas2020}, there remains a paucity of research in the matrix-variate setting. Matrix-variate or three-way data consists of matrices organized in three dimensions. This structure commonly arises in multivariate longitudinal data, with multiple measurements taken at different time points, and greyscale image data. Model-based clustering has demonstrated its effectiveness in clustering three-way data by leveraging mixtures of matrix-variate distributions \citep{viroli2011, anderlucci2015, dogru2016, gallaugher2018, silva2023}.

In response to the growing interest in matrix-variate data applications, this work develops an EA for matrix-variate model-based clustering. The algorithm incorporates both crossover and mutation, and is applied in the matrix-variate normal setting. We find that its performance is competitive against the EM algorithm for clustering three-way data in both simulated and real-world datasets.   



%---------------------------%
%      CHAPTER 2      %
%---------------------------%
\section{Background}
%--------------------------------------------------%
\subsection{Matrix-Variate Normal Distribution}
%--------------------------------------------------%
Three-way datasets are characterized by having $n$ units (rows), $p$ variables (columns), and $N$ occasions (layers). To effectively model such data, we use matrix-variate distributions, the most mathematically tractable being the matrix-variate normal distribution \citep{gupta1999}. An $n \times p$ random matrix $\mathscr{X}$ follows an $n \times p$ matrix-variate normal distribution, denoted $\mathcal{N}_{n\times p}({\bf M, \Sigma, \Psi})$, if its density can be written as
\begin{equation*}
\phi_{n\times p}({\bf X |  M, \Sigma, \Psi}) =  \frac{1}{(2\pi)^{\frac{np}{2}}|{\bf \Sigma}|^{\frac{p}{2}}|{\bf \Psi}|^{\frac{n}{2}}} \exp\left\{ -\frac{1}{2}\text{tr}\left({\bf \Sigma}^{-1}({\bf X} - {\bf M}) {\bf \Psi}^{-1}({\bf X} - {\bf M}) '\right)\right\},
\end{equation*}
where $\bf M$ is the $n \times p$ location matrix, $\bf \Sigma$ is the $n \times n$ row covariance matrix, and $\bf \Psi$ is the $p \times p$ column covariance matrix. An equivalent formulation in the multivariate setting is given by
\begin{equation*}
\mathscr{X} \sim \mathcal{N}_{n\times p}({\bf M, \Sigma, \Psi}) \iff \text{vec}(\mathscr{X}) \sim \mathcal{N}_{np}(\text{vec}({\bf M}), {\bf \Psi} \otimes {\bf \Sigma}),
\end{equation*}
where $\mathcal{N}_{np}(\cdot)$ is the $np$ multivariate normal distribution, $\text{vec}(\cdot)$ is the vectorization operator, and $\otimes$ is the Kronecker product. 


%--------------------------------------------------%
\subsection{Model-Based Clustering}
%--------------------------------------------------%
Classification is a paradigm in which group membership labels are assigned to unlabelled data. Unsupervised classification, or clustering, assumes that all observations are unlabelled or are treated as such. One common method of clustering is model-based, which involves maximizing the likelihood of a $G$-component finite mixture model. A matrix-variate random variable $\mathscr{X}$ arises from a finite mixture model if its density can be written as
\begin{equation*}
f({\bf X} | \bm{\vartheta}) = \sum_{g=1}^G \pi_g f_g({\bf X} | \bm{\theta}_g),
\end{equation*}
where $f_g(\cdot)$ is the gth component density, $\pi_g >0$ is the gth mixing proportion with $\sum_{g=1}^G\pi_g = 1$, and $\bm{\vartheta} = (\bm{\pi}, \bm{\theta}_1, \bm{\theta}_2, \dots, \bm{\theta}_G)$ is the vector of parameters with $\bm{\pi} = (\pi_1, \pi_2, \dots, \pi_G)$. In most applications, the component density functions are taken to be identical conditioned on $\bm{\theta}_g$ and Gaussian, that is, $f_g({\bf X} | \bm{\theta}_g) = \phi({\bf X} | \bm{\theta}_g)$ for all $g$.

\citet{mcnicholas2016b} traces the framing of a cluster in terms of a component of a mixture model back to \citet{tiedeman1955}, and the earliest use of a mixture model for clustering to \citet{wolfe1965}, who uses a multivariate Gaussian mixture model. A recent review of model-based clustering can be found in \citet{mcnicholas2016b}, while extensive details are available in \citet{mcnicholas2016a}. 


%-------------------------------------------------%
\subsection{The EM Algorithm}
%-------------------------------------------------%
Estimation of the mixture model parameters is typically performed using the expectation-maximization (EM) algorithm \citep{dempster1977}. Consider a dataset comprising $N$ unlabelled matrices ${\bf X}_1, {\bf X}_2, \dots, {\bf X}_N$ of size $n \times p$. Then the observed-data likelihood for a finite matrix-variate normal mixture is given by
\begin{equation*}
\mathcal{L}(\bm{\vartheta}) = \prod_{i=1}^N \sum_{g=1}^G\pi_g \phi({\bf X}_i | {\bf M}_g, {\bf \Sigma}_g, {\bf \Psi}_g).
\end{equation*} 
To facilitate clustering, we introduce latent membership indicators $z_{ig}$ where 
\begin{equation*}
z_{ig} = 
\begin{cases}
    1 & \text{if observation ${\bf X}_i$ belongs to component $g$,} \\
    0 & \text{otherwise},
    \end{cases}
\end{equation*} 
for $i= 1, \dots, N; g = 1, \dots, G$. The indicators can be organized into row vectors ${\bf z}_1, {\bf z}_2, \dots, {\bf z}_N$ where ${\bf z}_i = (z_{i1}, z_{i2}, \dots, z_{iG})$ denotes the membership label for data point $i$, and further arranged into an $N \times G$ solution matrix denoting a clustering of the data 
\begin{equation*}
{\bf \mathbfcal{Z}} = 
\begin{bmatrix}
{\bf z}_1 \\
{\bf z}_2 \\
\vdots \\
{\bf z}_N \\
 \end{bmatrix} 
 = 
 \begin{bmatrix}
    z_{11} & z_{12} & \dots & z_{1G} \\
    z_{21} & z_{22}& \dots & z_{2G}\\
    \vdots & \vdots & \ddots & \vdots \\
    z_{N1}& z_{N2}& \dots & z_{NG}\\
\end{bmatrix}.
\end{equation*}
Using the data and membership labels, we obtain the complete-data log-likelihood
\begin{align*}
l_c(\bm{\vartheta}) &= \sum_{i=1}^N \sum_{g=1}^G z_{ig} \left[ \log \pi_g + \log \phi({\bf X}_i | {\bf M}_g, {\bf \Sigma}_g, {\bf \Psi}_g) \right].
\end{align*}
For each iteration of the EM algorithm, the E-step replaces the $z_{ig}$ indicators with their conditional expected values given the data and the current parameter estimates
\begin{equation*}
\hat{z}_{ig} = \mathbb{E}[z_{ig} | {\bf X}_i, \hat{\bm{\vartheta}}] = \mathbb{P}[z_{ig} = 1 | {\bf X}_i, \hat{\bm{\vartheta}}] = \frac{\hat{\pi}_g \phi(\hat{\bf {X}}_i |\hat{\bf {M}}_g, \hat{\bf {\Sigma}}_g, \hat{\bf{\Psi}}_g)}{\sum_{h=1}^G \hat{\pi}_h \phi(\hat{\bf {X}}_i |\hat{\bf {M}}_h, \hat{\bf {\Sigma}}_h, \hat{\bf{\Psi}}_h)}.
\end{equation*}
The predictions are reported as either soft or hard classifications. In soft classification, the values of $\hat{z}_{ig}$ remain as probabilities in $(0, 1)$, as computed during the E-step. Alternatively, hard classification reports the maximum a posteriori (MAP) classification i.e, $\text{MAP}\{ \hat{z}_{ig}\}$, where 
\begin{equation*}
\text{MAP}\{ \hat{z}_{ig}\} = 
\begin{cases}
    1 & \text{if $g = \text{argmax}_h \{ \hat{z}_{ih}\}$}, \\
    0 & \text{otherwise}.
\end{cases}
\end{equation*}
In the M-step, the parameters are updated. \citet{viroli2011} derives the matrix-variate normal maximum likelihood estimates as
\[
\begin{aligned}
\hat{\pi}_g & = \frac{N_g}{N}, \quad N_g = \sum_{i=1}^N \hat{z}_{ig}, \\
\hat{{\bf M}}_g & = \frac{\sum_{i=1}^N \hat{z}_{ig}{\bf X}_i}{N_g}, \\
\hat{{\bf \Sigma}}_g & = \frac{\sum_{i=1}^N \hat{z}_{ig}({\bf X}_i - \hat{\bf M}_g) \hat{\bf \Psi}_g^{-1}({\bf X}_i - \hat{\bf M}_g)'}{pN_g}, \\
\hat{{\bf \Psi}}_g & = \frac{\sum_{i=1}^N \hat{z}_{ig}({\bf X}_i - \hat{\bf M}_g) \hat{\bf \Sigma}_g^{-1}({\bf X}_i - \hat{\bf M}_g)'}{nN_g},
\end{aligned}
\]
for $g = 1, \dots, G$. The E and M steps iterate until some convergence criterion is satisfied. We use a criterion based on the Aitken acceleration \citep{aitken1926} defined by 
\begin{equation*}
a^{(t)} = \frac{l^{(t+1)} - l^{(t)} }{l^{(t)} - l^{(t-1)}},
\end{equation*}
where $l^{(t)}$ is the observed log-likelihood at iteration $t$. From this, \citet{lindsay1995} and \citet{bohning1994} calculate the asymptotic estimate of the log-likelihood at iteration $t+1$ as
\begin{equation*}
l_{\infty}^{(t+1)} = l^{(t)} + \frac{l^{(t+1)} - l^{(t)}}{1- a^{(t)}}.
\end{equation*}
In accordance with \citet{mcnicholas2010}, the EM algorithm terminates when 
\begin{equation*}
0< l_{\infty}^{(t+1)} - l^{(t)} < \varepsilon,
\end{equation*}
for some pre-specified tolerance $\varepsilon > 0$.


%-----------------------------------------%
\subsection{Model Selection and Performance}
%-----------------------------------------%
In general, the number of underlying components or clusters is a priori unknown. To select the appropriate number, we use the Bayesian information criterion \citep[BIC;][]{schwarz1978} defined by 
\begin{equation*}
\text{BIC} = 2 l(\hat{\bm{\vartheta}}) - 2 \rho \log N,
\end{equation*}
where $l(\hat{\bm{\vartheta}})$ is the maximized log-likelihood, $N$ is the number of observations, and $\rho$ is the number of free parameters. Arguments for the use of the BIC for mixture-models can be found in \citet{leroux1992}, \citet{keribin2000}, and \citet{dasgupta1998}.

Model performance is based on the adjusted Rand index \citep[ARI;][]{hubert1985} which quantifies the pairwise agreements between two sets of class assignments after accounting for agreement by chance. Under random assignment, the expected ARI is 0, while a value of 1 is achieved for perfect assignment. Negative values indicate worse than random assignment.

%--------------------------------------------------%
\subsection{Benefits Over Vectorization}
%--------------------------------------------------%
When dealing with matrix-variate clustering, it is tempting to revert to the multivariate setting through vectorization, permitting the use of well-established techniques. However, matrix-variate analysis often offers a substantial reduction in the number of free scale parameters. \citet{gallaugher2018} demonstrates a free scale parameter reduction from $(n^2p^2 + np)/2$ to $(n^2 + p^2 + n + p)/2$, which simplifies and speeds up the procedure for almost all values of $n$ and $p$. Therefore, it is profitable to remain in the matrix-variate setting when dealing with three-way data. 




%-------------------------------------------%
\subsection{Evolutionary Algorithms}
%-------------------------------------------%
Evolutionary algorithms (EA) exploit the principles of biological evolution for global optimization. They explore the solution space without assuming any prior knowledge of its structure making them remarkably effective against challenging optimization problems. This treatment focuses on the perspective detailed in \citet{ashlock2010}. The goal of an EA is to maximize a fitness function by evolving a population of candidate solutions. In contrast to other optimization techniques that yield only a single solution, EAs offer the advantage of obtaining a population of optimal solutions. 

To begin, the first generation of candidate solutions is initialized. For each generation, genetic operators such as crossover and mutation are applied to the population members giving rise to new individuals. Only the fittest individuals among the population survive ensuring the propagation of advantageous traits. This process continues until the population stagnates to a collection of maximally fit solutions. 

The principal mechanism for reproduction is the crossover operator, which randomly exchanges genetic material between two or more parent solutions to create at least one offspring. This promotes exploration of the solution space by creating new and diverse candidate solutions. Over many generations, the offspring inherit favourable traits from their parents leading to fitter solutions. Alternatively, mutation is a genetic operator that acts on individual population members by randomly introducing slight genetic modifications. This prevents premature convergence to suboptimal populations that may result from crossover alone. 

Early applications of EAs for model-based clustering can be found in \citet{martinez2000} and \citet{pernkopf2005}, which focus on evolving the parameter space. More recently, the works of \citet{andrews2013} and \citet{mcnicholas2020} have evolved a population of latent cluster membership matrices. The main advantage of evolving membership labels instead of the parameters lies in the size of the respective spaces. The total number of possible classifications of $N$ observations into $G$ groups is finite, in contrast to the infinite parameter space \citep{andrews2013}. In this work, we aim to extend the crossover and mutation based multivariate Gaussian EA developed in \citet{mcnicholas2020} to the matrix-variate setting, allowing for broader applicability.


%----------------------------------------------%
\section{Methodology}
%----------------------------------------------%


%----------------------------------------------%
\subsection{Model and Fitness Function}
%----------------------------------------------%
The underlying model for our EA is a mixture of matrix-variate normal distributions. Representing individual clusterings of the data, matrices $\mathbfcal{Z}_k$ consisting of indicator variables $z_{ig}$ serve as population members. Unlike the EM algorithm which uses soft clustering, the EA estimates $z_{ig}$ using hard clustering. For clarity, we denote $\hat{z}_{ig} \in [0, 1]$ for the EM estimates and $\tilde{z}_{ig} \in \{0, 1\}$ for the EA estimates. To compute the fitness of an individual solution, we first update the matrix-variate normal model parameters, replacing $\hat{z}_{ig}$ with $\tilde{z}_{ig}$, that is,
\[
\begin{aligned}
\tilde{\pi}_g & = \frac{N_g}{N}, \quad N_g = \sum_{i=1}^N \tilde{z}_{ig}, \\
\tilde{{\bf M}}_g & = \frac{\sum_{i=1}^N \tilde{z}_{ig}{\bf X}_i}{N_g}, \\
\tilde{{\bf \Sigma}}_g & = \frac{\sum_{i=1}^N \tilde{z}_{ig}({\bf X}_i - \tilde{\bf M}_g) \tilde{\bf \Psi}_g^{-1}({\bf X}_i - \tilde{\bf M}_g)'}{pN_g}, \\
\tilde{{\bf \Psi}}_g & = \frac{\sum_{i=1}^N \tilde{z}_{ig}({\bf X}_i - \tilde{\bf M}_g) \tilde{\bf \Sigma}_g^{-1}({\bf X}_i - \tilde{\bf M}_g)'}{nN_g},
\end{aligned}
\]
for $g = 1, \dots, G$. Then we calculate the observed log-likelihood at the current EA estimates  
\begin{equation*}
\text{fitness}(\tilde{\bf \mathbfcal{Z}}_k) = \sum_{i=1}^N \log \left (\sum_{g=1}^G\pi_g f({\bf X}_i | \tilde{\bf M}_g, \tilde{\bf \Sigma}_g, \tilde{\bf \Psi}_g)\right).
\end{equation*} 




%--------------------------------%
\subsection{Algorithm Design}
%--------------------------------%
The EA starts by initializing the first generation with a population of $K$ parent solutions, denoted as $\tilde{\bf \mathbfcal{Z}}_1, \dots, \tilde{\bf \mathbfcal{Z}}_K$. In general, the parent solutions are randomly generated matrices representing random hard clusterings of the data. Alternatively, one can provide initial solutions, such as a hardened $k$-means or other handpicked solutions, to inform the evolutionary process.  

Reproduction in the EA involves cloning the parents $J$ times and performing crossover on each parent and clone. Crossover randomly exchanges two distinct rows between the parent solution and its clone creating an offspring with identical membership labels except for two observations. It's worth noting that this exchange can also be viewed as a single parent operation in which two distinct rows of a single parent are swapped. While this differs from the exploratory aspect of biological crossover, it offers the advantage of monotonically increasing the fitness of the population. To illustrate this operation, suppose we randomly select labels $\tilde{\bf z}_2$ and $\tilde{\bf z}_N$ from the parent in Figure \ref{fig:crossover}. Since the rows are distinct, we swap them to create offspring $\tilde{ \bf \mathbfcal{Z} }_{k_j}$ with the same genetic material except for those two labels.   
\begin{figure}[H]
\setstretch{1.5}
$$\tilde{\bf {\mathbfcal{Z}}}_{k} = 
\begin{bmatrix} \tilde{z}_{11} & \tilde{z}_{12} & \dots & \tilde{z}_{1G} \\
    \textcolor{blue}{0} & \textcolor{blue}{0} & \dots & \textcolor{blue}{1} \\
    \vdots & \vdots & \ddots & \vdots \\
    \textcolor{red}{0}&\textcolor{red}{1}& \dots & \textcolor{red}{0}\\
\end{bmatrix} 
\;
\;
\;
\xrightarrow{\text{crossover}}
\;
\;
\;
\tilde{ \bf \mathbfcal{Z} }_{k_j} = 
\begin{bmatrix}
    \tilde{z}_{11} & \tilde{z}_{12} & \dots & \tilde{z}_{1G} \\
    \textcolor{red}{0} & \textcolor{red}{1} & \dots & \textcolor{red}{0} \\
    \vdots & \vdots & \ddots & \vdots \\
    \textcolor{blue}{0}&\textcolor{blue}{0}& \dots & \textcolor{blue}{1}\\
\end{bmatrix}   $$
\vspace{0.5cm}
\setstretch{1}
\caption{Illustration of the crossover operator, randomly swapping distinct rows $\tilde{\bf z}_2$ and $\tilde{\bf z}_N$ from the parent solution to create a child with similar genetic material.}
\label{fig:crossover}
\end{figure}

If the labels are identical, we continue randomly selecting rows until two distinct labels are found. After crossover, the population size grows to $K + KJ$, including the original parents and their offspring. The population is then organized into a list of descending fitness, from which we select the top $K$ solutions to become the next generation of parents. This crossover step helps avoid stopping at local maxima of the fitness surface, i.e., the observed log-likelihood. 

Because swapping alone does not guarantee better clustering results, we now introduce a greedy mutation step. For each of the surviving $K$ individuals, we randomly swap the unit-element in a random row until their fitness improves. Suppose label $\tilde{\bf z}_2$ is randomly selected from the parent in Figure \ref{fig:mutation}. Within this row, we may swap the unit-element $\tilde{z}_{2G}=1$ with randomly selected zero-element $\tilde{z}_{22}=0$.
\vspace{0.5cm}
\begin{figure}[H]
\setstretch{1.5}
$$\tilde{ {\bf \mathbfcal{Z}} }_{k}  = 
\begin{bmatrix}
    \tilde{z}_{11} & \tilde{z}_{12} & \dots & \tilde{z}_{1G} \\
    \tilde{z}_{21} & \textcolor{red}{0} & \dots & \textcolor{blue}{1} \\
    \vdots & \vdots & \ddots & \vdots \\
    \tilde{z}_{N1}& \tilde{z}_{N2}& \dots & \tilde{z}_{NG}\\
\end{bmatrix} 
\;
\;
\;
\xrightarrow{\text{mutation}}
\;
\;
\;
\tilde{ {\bf \mathbfcal{Z}} }_{k} = 
\begin{bmatrix}
    \tilde{z}_{11} & \tilde{z}_{12} & \dots & \tilde{z}_{1G} \\
    \tilde{z}_{21} & \textcolor{blue}{1} & \dots & \textcolor{red}{0} \\
    \vdots & \vdots & \ddots & \vdots \\
    \tilde{z}_{N1}& \tilde{z}_{N2}& \dots & \tilde{z}_{NG}\\
\end{bmatrix}$$
\vspace{0.5cm}
\setstretch{1}
\caption{Illustration of the mutation operator, swapping unit-element $\tilde{z}_{2G}$ with zero-element $\tilde{z}_{22}$ in a random row of a surviving parent for a slightly modified solution.}
\label{fig:mutation}
\end{figure}

If the mutation increases fitness, the swap remains; otherwise, the swap is reverted. This process continues until either a profitable mutation is found or all rows have been exhausted leaving the parent unchanged. If a generation remains unchanged after applying both crossover and mutation, it is considered a stagnation. After a predetermined number of consecutive stagnations, the algorithm terminates, resulting in a population of $K$ fit solution matrices.




%-------------------------%
\begin{algorithm}[H]
%-------------------------%
\caption{EA for matrix-variate model-based clustering.}
\begin{algorithmic}[1]
\setstretch{1.15} 
	\Statex \textbf{Input:}
     		\Statex $ {\mathbfcal{X}} = [{\bf X}_1, \dots, {\bf X}_N ]\gets$ $n \times p \times N$ array of observations
		\Statex $ \tilde{ {\bf \mathfrak{Z} }} = [\tilde{ {\mathbfcal{Z}}}_{1}, \dots, \tilde{ {\mathbfcal{Z}}}_{K}]  \gets$ $N \times G \times K$ array of membership labels \Comment{random if not specified}
		 \Statex $G \gets$ number of clusters
   		 \Statex $K \gets$ number of parents
    		 \Statex $J \gets$ number of clones
    		 \Statex $S \gets$ max number of stagnations
\State s = 0
\While{$s < S$}

	\For{$k = 1 \text{ to } K$} \Comment{crossover step}
		\For{$j = 1 \text{ to } J$}
		        \State \textbf{Crossover:} randomly swap two distinct labels from parent $\tilde{ {\bf \mathbfcal{Z}}}_{k}$ to get offspring $\tilde{ {\bf \mathbfcal{Z}}}_{kj}$
       			 \State \textbf{Fitness:} update model parameters and calculate log-likelihood of $\tilde{ {\bf \mathbfcal{Z}}}_{kj}$
		\EndFor
	\EndFor
	
	\State \textbf{Survival:} sort parents and offspring by descending fitness and take top $K$ as new parents 
		
	\For{$k = 1 \text{ to } K$} \Comment{mutation step}
         	\For{$r$ in random permutation of $1$ to $N$} 
        			\State \textbf{Mutate:} randomly swap unit-element with zero-element in row $r$ 
     			\If{fitness increases}
        				\State \textbf{break for}
     			\Else
     				\State revert the unit-element and zero-element swap 
   			 \EndIf
        		\EndFor
        \EndFor  
        \If{parents are identical to last generation}
        \State \texttt{s} $\gets$  $\texttt{s}  + 1$
        \Else
        \State \texttt{s} $\gets 0$ 
    \EndIf
\EndWhile
\Statex \textbf{Return} final population $\tilde{ {\bf \mathfrak{Z} }}$
\end{algorithmic}
\end{algorithm}

\newpage



%-----------------------%
\subsection{Procedure}
%-----------------------%
The subsequent data analysis adopts the clustering paradigm, treating all membership labels as unknown. For each dataset, two matrix-variate normal mixtures are applied: one using the EM algorithm for parameter estimation and the other using our EA. Both algorithms are initialized with a random start. Let $\mathcal{G}$ be the true number of classes, then the algorithms are run for $G=2,\dots, \mathcal{G} + 1$. The value of $G$ which yields the largest BIC is selected. In addition to selecting $G$, the EA must also determine the number of parents $K \in \{1, 2, 3\}$, and the number of clones $J \in \{4, 8, 12\}$. The number of parents is relatively low to ensure that mutations are applied sparingly, while the number of children is large to encourage crossover. For convergence, the EM algorithm uses the Aitken's based criterion with a tolerance of $\varepsilon =10^{-6}$, while the EA uses $S=3$ stagnations. Model performance is based on the final converged log-likelihood, AIC, total runtime, and EA to EM likelihood ratio. The EM algorithm is implemented using the \texttt{R} package \citet{matrixmixtures} and the EA is available in \texttt{Julia} at \citet{flynn2023} \citep{R, Julia}.





%------------------------%
\section{Simulation Study}
%------------------------%
Two simulations are conducted by clustering a collection of datasets generated from mixtures of matrix-variate normals. In each simulation, a total of 25 datasets are created from the same set of arbitrarily selected model parameters. Simulation 1 involves $3 \times 4$ data consisting of $G = 2$ true classes and a total of $N=300$ observations. Simulation 2 considers $4 \times 3$ data generated from $\mathcal{G} = 3$ true classes, totalling $N= 300$ observations. All datasets are balanced, with equal proportions in each class. The groups in Simulation 1 are well-separated, whereas the groups in Simulation 2 are more challenging to distinguish. 



%------------------------%
\subsection{Simulation 1}
%------------------------%
In Simulation 1, the location parameters set to
$$
{\bf M}_1 =
\begin{bmatrix} 
1  &  0 & 1  & -1 \\
-1 & -1 &  1 & 0\\
 0 & 0  & 1 & -1
\end{bmatrix}, \quad 
{\bf M}_2 =
\begin{bmatrix} 
0  &  -1 & 1  & 0 \\
-1 & 0 &  0 & 1\\
 1 & 0  & 1 & -1
\end{bmatrix},
$$
and the scale parameters are given by
\begin{align*}
{\bf \Sigma}_1 &=
\begin{bmatrix} 
1  &  0.4 & 0.75   \\
0.4 & 1 &  0\\
 0.75 & 0  & 1 
\end{bmatrix}, & 
{\bf \Sigma}_2 &=
\begin{bmatrix} 
1 &  0.6 & 0.25  \\
0.6 & 1 &  0.1 \\
 0.25 & 0.1  & 1 
\end{bmatrix},
\\
{\bf \Psi}_1 &=
\begin{bmatrix} 
      1  &      0 &   0.35  & 0.15 \\
      0 &       1 &    0      & 0.85\\
 0.35 &       0  &   1      & 0\\
 0.15 &   0.85  &   0     & 1
\end{bmatrix}, & 
{\bf \Psi}_2 &=
\begin{bmatrix} 
      1  &      0.2 &     0  & 0.6 \\
      0.2 &       1 &       0.55      & 0\\
      0 &        0.55  &   1      & 0.3\\
   0.6 &       0  &             0.3     & 1
\end{bmatrix}.
\end{align*}

The optimal number of components for the EM algorithm was $G=2$ and the optimal EA consisted of $G=2$, $K=1$, and $J=12$. Table \ref{table:sim1} presents the mean and standard deviation for ARI, runtime, and EA to EM likelihood ratio of the optimal models across the 25 runs. The results indicate that both the EM and EA exhibited nearly identical performance. While the EA found slightly superior maxima, as evidenced by the average likelihood ratio, the EM achieved slightly better ARI scores. We acknowledge that the EA is generally slower than the EM, however, the runtimes appear comparable since the number of parents is quite low.

\begin{table}[!htbp]
  \caption{Mean and standard deviation of ARI, runtime, and likelihood ratio for EA and EM associated with Simulation 1.}
  \label{table:sim1}
  \begin{tabularx}{\textwidth}{l *{3}{X}c}
  \toprule
    &\textbf{ARI}& \textbf{Runtime (sec)}  & \textbf{Likelihood Ratio} \\
  \midrule
  EA & 0.992 (0.010)     & 1.56 (0.19) & \multirow{2}{*}{1.001 (0.005)}  \\
  EM & 0.993 (0.008)    & 0.77 (0.06)   \\
  \bottomrule
  \end{tabularx}
\end{table}



%------------------------%
\subsection{Simulation 2}
%------------------------%
The location parameters for simulation 2 are given by
$$
{\bf M}_1 =
\begin{bmatrix} 
0  &  0.5 & 1   \\
0.5 & 1 &  0.5 \\
0.5 & 1 &  0.5 \\
 0 & 1 & 0
\end{bmatrix}, \quad 
{\bf M}_2=
\begin{bmatrix} 
1 &  0.5 & 0   \\
1.5 & 1 &  2 \\
0 & 2 &  0.5 \\
 1.5 & 0.5 & 1
\end{bmatrix},
\quad 
{\bf M}_3=
\begin{bmatrix} 
1.5 &  2.5 & 2   \\
1 & 3 &  1.5 \\
0.5 & 3 &  1.5 \\
 1.5 & 0.5 & 1
\end{bmatrix},
$$
and the scale parameters configured to
\begin{align*}
{\bf \Sigma}_1 = {\bf \Sigma}_3 &=
\begin{bmatrix} 
1 &     0.1&     0.45&        0.1\\
                0.1&     1&     0.25&    0.35\\
                0.45&       0.25&     1&     0.1\\
                0.1&      0.35&       0.1&       1\\
\end{bmatrix},&  
{\bf \Sigma}_2 &=
\begin{bmatrix} 
      1  &      0.2 &     0  & 0.6 \\
      0.2 &       1 &       0.55      & 0\\
      0 &        0.55  &   1      & 0.3\\
   0.6 &       0  &             0.3     & 1
\end{bmatrix},
\\
{\bf \Psi}_1 &=
\begin{bmatrix} 
1  &  0.4 & 0.75   \\
0.4 & 1 &  0\\
 0.75 & 0  & 1 
\end{bmatrix},  &
{\bf \Psi}_2= {\bf \Psi}_3 &= 
\begin{bmatrix} 
1  &  0.5 & 0.5   \\
0.5 & 1 &  0\\
 0.5 & 0.5  & 1 
\end{bmatrix}.
\end{align*}
The number of components selected for the EM algorithm was $G=3$, while the optimal EA was identified with $G=3$, $K=3$, and $J=12$. Table \ref{table:sim2} presents similar results to the previous simulation, with the EA exhibiting marginally superior performance in terms of likelihood, while the EM slightly outperformed in ARI scores. Notably, the larger number of parents resulted in significantly longer runtimes due to the greedy nature of the mutation step.

\begin{table}[!ht]
  \caption{Mean and standard deviation of ARI, runtime, and likelihood ratio for EA and EM associated with Simulation 2.}
      \label{table:sim2}
  \begin{tabularx}{\textwidth}{l *{3}{X}c}
  \toprule
    &\textbf{ARI}& \textbf{Runtime (sec)}  & \textbf{Likelihood Ratio} \\
  \midrule
  EA & 0.930 (0.032)     & 14.72 (1.82) & \multirow{2}{*}{1.041 (0.101)}  \\
  EM & 0.942 (0.026)    & 1.83 (0.42)   \\
  \bottomrule
  \end{tabularx}
\end{table}



%---------------------------%
%      CHAPTER 5      %
%---------------------------%



%------------------------%
\section{Application}
%------------------------%
Two real-world datasets are used for clustering. First is the Landsat satellite dataset, which involves $3 \times 3$ digital images of the same regions taken in four different spectral bands. The pixels are arranged into matrices of size $4 \times 9$ and three classes are used. The second dataset contains $16 \times 16$ greyscale images of handwritten digits. We focus on clustering digits 1 and 7.  





%-------------------------------------------%
\subsection{Landsat Satellite Dataset}
%-------------------------------------------%
This dataset is a collection of digital images captured by the Landsat program, which is a series of Earth-observing satellites managed by NASA and the United States Geological Survey (USGS). The original data was purchased from NASA by the Australian Centre for Remote Sensing and subsequently donated to the UCI machine learning repository in a preprocessed form \citep{landsat}. 

Four digital images of size $3 \times 3$ were taken of the same region in different spectral bands, corresponding to random matrices of size $4\times 9$. Two of the bands were in the visible spectrum, while the other two were in the infrared spectrum. The pixel intensities range from 0 to 255. The researchers performed site visits to identify seven classes of images based on the four central pixels, which are described in Table \ref{table:classes}. Note that they removed class 6 due to doubts of its validity.

\begin{table}[!htbp]
  \caption{Description of the Landsat test set classes and observation count.}
    \label{table:classes}
  \begin{tabularx}{\textwidth}{cXc}
    \toprule
    \textbf{Class} & \textbf{Description} & \textbf{Count} \\
    \midrule
    1 & Red soil                        & 461 \\
    2 & Cotton crop                     & 224 \\
    3 & Grey soil                       & 396 \\
    4 & Damp grey soil                  & 211 \\
    5 & Soil with vegetation stubble    & 237 \\
    6 & Mixture of all classes          & 0 \\
    7 & Very damp grey soil             & 470 \\
    \bottomrule
  \end{tabularx}
\end{table}

For our analysis, we focused on the first three classes from the test set: red soil, cotton crop, and grey soil, resulting in a dataset of size $N = 1081$ observations with true proportions approximately $\bm{\pi} = (0.43, 0.21, 0.37)$. Surprisingly, both the EM and EA resulted in $G=4$ components. The presence of this extra class could be attributed to the lack of distinct borders between classes of images, as the images were labeled based on their central pixels. In fact, it is possible that these bordering regions should have been assigned to class 6, which was later removed. The optimal EA also comprised $K=2$ parents and $J=8$ clones. Table \ref{table:landsat_results} showcases the results, including the log-likelihood, ARI, total runtime, and the EA to EM likelihood ratio. Additionally, Tables \ref{table:landsat_EA} and \ref{table:landsat_EM} present the cross-tabulation of the MAP classifications for the EA and EM, respectively. The results demonstrate that, despite an increase of approximately 5 minutes in runtime, the EA discovered a superior maximum. Moreover, an EA to EM likelihood ratio of approximately 1.55 indicates an improvement in likelihood. This increase in EA likelihood translated to a slight increase in ARI over the EM. Notably, both algorithms exhibit a similar clustering pattern in the cross-tabulations.


\begin{table}[!htbp]
  \caption{Converged log-likelihood, ARI, runtime, and EA to EM likelihood ratio associated with the Landsat dataset.}
      \label{table:landsat_results}
  \begin{tabularx}{\textwidth}{l *{3}{X}c}
  \toprule
    &\textbf{Log-Likelihood}& \textbf{ARI} & \textbf{Runtime (sec)} & \textbf{Likelihood Ratio} \\
  \midrule
   EA & -108118.26     & 0.878 &  348.48 & \multirow{2}{*}{1.55}\\
  EM & -108118.7          & 0.869 & 36.89    \\
  \bottomrule
  \end{tabularx}
\end{table}

\begin{table}[!htbp]
  \caption{Cross-tabulation of the EA MAP classifications associated with the Landsat dataset.}
      \label{table:landsat_EA}
  \begin{tabularx}{\textwidth}{l *{4}{X}}
  \toprule
   &\textbf{Cluster 1}  & \textbf{Cluster 2}  & \textbf{Cluster 3}  & \textbf{Cluster 4}   \\
  \midrule
  Red Soil & 	452 & 0 & 0 & 9\\
  Cotton Crop &0 & 140 &  0 & 84 \\
   Grey Soil &   7 & 0 &  368  & 21\\
  \bottomrule
  \end{tabularx}
\end{table}

\begin{table}[!htbp]
  \caption{Cross-tabulation of the EM MAP classifications associated with the Landsat dataset.}
      \label{table:landsat_EM}
  \begin{tabularx}{\textwidth}{l *{4}{X}}
  \toprule
   &\textbf{Cluster 1}  & \textbf{Cluster 2}  & \textbf{Cluster 3}  & \textbf{Cluster 4}   \\
  \midrule
  Red Soil & 	450 & 0 & 0 & 11\\
  Cotton Crop &0 & 150 &  0 & 74 \\
   Grey Soil &   7 & 0 &  363  & 26\\
  \bottomrule
  \end{tabularx}
\end{table}


%--------------------------------------------%
\subsection{Handwritten Digits Dataset}
%--------------------------------------------%
The final dataset comprises greyscale images of handwritten digits, which were obtained by scanning envelopes from the U.S postal service. To facilitate analysis, the Elements of Statistical Learning provides preprocessed $16 \times 16$ images with pixel intensities between -1 and 1 \citep{hastie2009}. 

Since the writing is mostly concentrated in the center, the outer rows and columns primarily contain white space, resulting in a value of -1. Consequently, the lack of variation along the borders leads to singular updates in the scale matrices ${\bf \Sigma}_g$ and ${\bf \Psi}_g$. To address this sparsity, we introduced small random noise to the images. However, to preserve the signal integrity, a constant is applied to all entries greater than -1 before adding the noise.

Our analysis focused on clustering digits 1 and 7 from the dataset. There were a total of $N=411$ observations from the test set, with 264 belonging to digit 1 and 147 to digit 7, corresponding to true proportions of approximately $\bm{\pi} = (0.64, 0.36)$. The optimal number of components for the EM algorithm was $G=2$, while the optimal EA consisted of $G=2, K=2,$ and $J=4$. In Table \ref{table:digits_results}, we present the log-likelihood, ARI, runtime, and EA to EM likelihood ratio. Additionally, Tables \ref{table:digits_EA} and \ref{table:digits_EM} display the cross-tabulation of EA and EM results, respectively. Remarkably, both algorithms showed identical clustering results in terms of likelihood and ARI. The cross-tabulations for EA and EM were also identical. However, we note that the EA required significantly more runtime to achieve the same results as the EM algorithm.


\begin{table}[!htbp]
  \caption{Converged log-likelihood, ARI, runtime, and EA to EM likelihood ratio associated with the handwritten digits dataset.}
        \label{table:digits_results}
  \begin{tabularx}{\textwidth}{l *{3}{X}c}
  \toprule
    &\textbf{Log-Likelihood}& \textbf{ARI} & \textbf{Runtime (sec)} & \textbf{Likelihood Ratio} \\
  \midrule
  EA & -97320.28     & 0.904 &  43.75 & \multirow{2}{*}{1.00}  \\
  EM & -97320.28    & 0.904 & 2.95  \\
  \bottomrule
  \end{tabularx}
\end{table}


\begin{table}[!htbp]
  \caption{Cross-tabulation of the EA MAP classifications associated with the handwritten digits dataset.}
        \label{table:digits_EA}
  \begin{tabularx}{\textwidth}{l *{4}{X}}
  \toprule
   && \textbf{Cluster 1}  & \textbf{Cluster 2}  \\
  \midrule
  Digit 1 && 256 & 8 \\
  Digit 7 && 2 &  145 \\
  \bottomrule
  \end{tabularx}
\end{table}

\begin{table}[!ht]
  \caption{Cross-tabulation of the EM MAP classifications associated with the handwritten digits dataset.}
          \label{table:digits_EM}
  \begin{tabularx}{\textwidth}{l *{4}{X}}
  \toprule
   && \textbf{Cluster 1}  & \textbf{Cluster 2} \\
  \midrule
  Digit 1 && 256 & 8 \\
  Digit 7 && 2 &  145 \\
  \bottomrule
  \end{tabularx}
\end{table}


%---------------------------%
%      CHAPTER 6      %
%---------------------------%


\newpage
%------------------------%
\section{Summary}
%------------------------%
This work developed an EA for matrix-variate normal mixtures using crossover and mutation. The EA was applied to simulated and real-world datasets and its performance was compared against the EM algorithm. Overall, the EA proves to be a competitive alternative for fitting model parameters in matrix-variate normal mixtures, consistently performing at least as well as the EM algorithm in terms of likelihood and ARI. For the Landsat data, the EA outperformed the EM showcasing its potential to handle complex and ill-defined problems more effectively. However, its limitation lies in its runtime, which is either as fast or longer than the EM algorithm. 

Future research could focus on improving the EA's runtime performance, either by exploring more optimal implementations of genetic operators or using parallel computing. Additionally, extending the investigation to non-Gaussian distributions could broaden the applicability of the proposed method. Lastly, exploring new genetic operators or modified versions of crossover and mutation could lead to further enhancements in the EA's performance.
\newpage

{\small 
%\section*{Acknowledgements}
\bibliographystyle{chicago}
\bibliography{references}}


\end{document}